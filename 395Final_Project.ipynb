{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6049ca99",
   "metadata": {},
   "source": [
    "1. Research Question: \n",
    "\n",
    "Can we accurately predict whether a movie review from Rotten Tomatoes expresses a positive or negative sentiment using machine learning techniques, and how does a deep learning model’s performance compare to traditional feature-based methods?\n",
    "\n",
    "\n",
    "2. Data Source:\n",
    "\n",
    "The dataset, titled Rotten Tomatoes Movies Reviews (available on Kaggle), contains short text reviews labeled as either positive or negative. Each observation includes the review text and a corresponding binary sentiment label. The dataset comprises thousands of reviews and is suitable for supervised text classification.\n",
    "\n",
    "\n",
    "3. Planned Methods:\n",
    "\n",
    "Our approach will follow a similar method as the spam classification challenge on homework #3, with an emphasis on feature engineering, pipeline encapsulation, and model selection. In the traditional machine learning portion, we will transform the raw text reviews into numeric features using scikit-learn's TfidfVectorizer(). We will pair this vectorization step with model training using a scikit-learn “pipeline” so that preprocessing occurs within each fold during cross-validation. We will perform hyperparameter tuning using GridSearchCV() and 5-fold cross-validation to find the best-performing traditional model. We plan to compare logistic regression, support vector machines, and XGBoost, optimizing hyperparameters such as regularization strength, kernel type, and tree depth. We'll evaluate the performance of the models using the F1-score and accuracy metrics.\n",
    "For the deep learning component, we will fine-tune a pre-trained transformer model, potentially DistilBERT from Hugging Face, for binary sentiment classification. This will pick up on deeper contextual relationships in language that are out of reach for the classical vectorization methods. By comparing model performance and interpretability, we would like to see if deep learning yields a considerable improvement compared to the classical pipeline-based method.\n",
    "\n",
    "\n",
    "4. References: \n",
    "\n",
    "https://www.kaggle.com/datasets/thedevastator/movie-review-data-set-from-rotten-tomatoes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825864a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "                                             reviews  labels\n",
      "0                  simplistic , silly and tedious .        0\n",
      "1  it's so laddish and juvenile , only teenage bo...       0\n",
      "2  exploitative and largely devoid of the depth o...       0\n",
      "3  [garbus] discards the potential for pathologic...       0\n",
      "4  a visually flashy but narratively opaque and e...       0\n",
      "\n",
      "Column names:\n",
      "Index(['reviews', 'labels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_rt.csv\")\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abc87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews    0\n",
      "labels     0\n",
      "dtype: int64\n",
      "count    10662.000000\n",
      "mean       115.156256\n",
      "std         51.199546\n",
      "min          5.000000\n",
      "25%         77.000000\n",
      "50%        112.000000\n",
      "75%        150.000000\n",
      "max        269.000000\n",
      "Name: reviews, dtype: float64\n",
      "labels\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "print(df['reviews'].str.len().describe())\n",
    "print(df['labels'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece29235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['reviews']\n",
    "y = df['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a140d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe\n",
      "xgboost OK, version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys, xgboost\n",
    "print(sys.executable)\n",
    "print(\"xgboost OK, version:\", xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b9d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 104 candidates, totalling 520 fits\n",
      "Test Accuracy: 0.7744960150023441\n",
      "Test F1: 0.7757575757575758\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1062\n",
      "           1       0.77      0.78      0.78      1071\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "params_grid = [\n",
    "    # Logistic Regression grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LogisticRegression(max_iter=1000)],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    },\n",
    "\n",
    "    # SVM grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LinearSVC()],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "        # no class_weight here unless using SVC()\n",
    "    },\n",
    "\n",
    "    # XGBoost grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [XGBClassifier(eval_metric='logloss')],\n",
    "        'clf__max_depth': [3, 5],\n",
    "        'clf__gamma': [0, 0.25]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    params_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "best = gs.best_estimator_\n",
    "\n",
    "y_test_pred = best.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2cce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7740271917487107\n",
      "Test F1: 0.7760223048327137\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77      1062\n",
      "           1       0.77      0.78      0.78      1071\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "best_lr = gs.best_estimator_\n",
    "\n",
    "y_test_pred = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
