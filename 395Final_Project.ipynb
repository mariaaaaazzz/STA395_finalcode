{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a8ff3f",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Rotten Tomatoes Reviews:\n",
    "## Classical Feature-Based Models vs. Transformer Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d6674",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "This project investigates binary sentiment classification on short movie reviews from Rotten Tomatoes. The goal is to compare classical feature-based machine learning methods (TF-IDF + linear classifiers) with a fine-tuned transformer model (DistilBERT) to evaluate differences in predictive performance and modeling capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb7250",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We construct a scikit-learn Pipeline combining TF-IDF vectorization with three candidate classifiers:\n",
    "\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Machine\n",
    "- XGBoost\n",
    "\n",
    "Hyperparameters are tuned via 5-fold cross-validation using GridSearchCV, optimizing for F1-score.\n",
    "\n",
    "\n",
    "For the deep learning approach, we fine-tune a pre-trained DistilBERT model using the Hugging Face Transformers library. The model is trained for two epochs with AdamW optimization and evaluated on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed87041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_rt.csv\")\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['labels'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['reviews']\n",
    "y = df['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d9f76",
   "metadata": {},
   "source": [
    "## Classical Feature-Based Models\n",
    "\n",
    "We construct a scikit-learn Pipeline combining TF-IDF vectorization with three candidate classifiers:\n",
    "\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Machine\n",
    "- XGBoost\n",
    "\n",
    "Hyperparameters are tuned via 5-fold cross-validation using GridSearchCV, optimizing for F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "params_grid = [\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LogisticRegression(max_iter=1000)],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LinearSVC()],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [XGBClassifier(eval_metric='logloss')],\n",
    "        'clf__max_depth': [3, 5],\n",
    "        'clf__gamma': [0, 0.25]\n",
    "    }\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    params_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb2c5a",
   "metadata": {},
   "source": [
    "## DistilBERT\n",
    "\n",
    "Fine-tuning a pre-trained DistilBERT model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    X_train.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RTDataset(train_encodings, y_train)\n",
    "test_dataset  = RTDataset(test_encodings, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54a6ba",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "| Model | Accuracy | F1 |\n",
    "|-------|----------|----|\n",
    "| Logistic Regression | 0.77 | 0.78 |\n",
    "| SVM | 0.77 | 0.78 |\n",
    "| XGBoost | 0.70 | 0.72 |\n",
    "| DistilBERT | 0.84 | 0.83 |\n",
    "\n",
    "The transformer-based model substantially outperforms classical feature-based approaches, suggesting that contextual language representations provide a measurable advantage over bag-of-words representations in short-text sentiment tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874035cd",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "All experiments use a fixed random seed (42) for train/test splits and cross-validation. Dependencies are listed in requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049ca99",
   "metadata": {},
   "source": [
    "1. Research Question: \n",
    "\n",
    "Can we accurately predict whether a movie review from Rotten Tomatoes expresses a positive or negative sentiment using machine learning techniques, and how does a deep learning model’s performance compare to traditional feature-based methods?\n",
    "\n",
    "\n",
    "2. Data Source:\n",
    "\n",
    "The dataset, titled Rotten Tomatoes Movies Reviews (available on Kaggle), contains short text reviews labeled as either positive or negative. Each observation includes the review text and a corresponding binary sentiment label. The dataset comprises thousands of reviews and is suitable for supervised text classification.\n",
    "\n",
    "\n",
    "3. Planned Methods:\n",
    "\n",
    "Our approach will follow a similar method as the spam classification challenge on homework #3, with an emphasis on feature engineering, pipeline encapsulation, and model selection. In the traditional machine learning portion, we will transform the raw text reviews into numeric features using scikit-learn's TfidfVectorizer(). We will pair this vectorization step with model training using a scikit-learn “pipeline” so that preprocessing occurs within each fold during cross-validation. We will perform hyperparameter tuning using GridSearchCV() and 5-fold cross-validation to find the best-performing traditional model. We plan to compare logistic regression, support vector machines, and XGBoost, optimizing hyperparameters such as regularization strength, kernel type, and tree depth. We'll evaluate the performance of the models using the F1-score and accuracy metrics.\n",
    "For the deep learning component, we will fine-tune a pre-trained transformer model, potentially DistilBERT from Hugging Face, for binary sentiment classification. This will pick up on deeper contextual relationships in language that are out of reach for the classical vectorization methods. By comparing model performance and interpretability, we would like to see if deep learning yields a considerable improvement compared to the classical pipeline-based method.\n",
    "\n",
    "\n",
    "4. References: \n",
    "\n",
    "https://www.kaggle.com/datasets/thedevastator/movie-review-data-set-from-rotten-tomatoes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825864a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "                                             reviews  labels\n",
      "0                  simplistic , silly and tedious .        0\n",
      "1  it's so laddish and juvenile , only teenage bo...       0\n",
      "2  exploitative and largely devoid of the depth o...       0\n",
      "3  [garbus] discards the potential for pathologic...       0\n",
      "4  a visually flashy but narratively opaque and e...       0\n",
      "\n",
      "Column names:\n",
      "Index(['reviews', 'labels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_rt.csv\")\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abc87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews    0\n",
      "labels     0\n",
      "dtype: int64\n",
      "count    10662.000000\n",
      "mean       115.156256\n",
      "std         51.199546\n",
      "min          5.000000\n",
      "25%         77.000000\n",
      "50%        112.000000\n",
      "75%        150.000000\n",
      "max        269.000000\n",
      "Name: reviews, dtype: float64\n",
      "labels\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "print(df['reviews'].str.len().describe())\n",
    "print(df['labels'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece29235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['reviews']\n",
    "y = df['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a140d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe\n",
      "xgboost OK, version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys, xgboost\n",
    "print(sys.executable)\n",
    "print(\"xgboost OK, version:\", xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b9d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 104 candidates, totalling 520 fits\n",
      "Test Accuracy: 0.7744960150023441\n",
      "Test F1: 0.7757575757575758\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1062\n",
      "           1       0.77      0.78      0.78      1071\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "params_grid = [\n",
    "    # Logistic Regression grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LogisticRegression(max_iter=1000)],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    },\n",
    "\n",
    "    # SVM grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [LinearSVC()],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "        # no class_weight here unless using SVC()\n",
    "    },\n",
    "\n",
    "    # XGBoost grid\n",
    "    {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "        'tfidf__min_df': [1, 2],\n",
    "        'tfidf__max_df': [0.9, 1.0],\n",
    "        'clf': [XGBClassifier(eval_metric='logloss')],\n",
    "        'clf__max_depth': [3, 5],\n",
    "        'clf__gamma': [0, 0.25]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    params_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "best = gs.best_estimator_\n",
    "\n",
    "y_test_pred = best.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2cce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7740271917487107\n",
      "Test F1: 0.7760223048327137\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77      1062\n",
      "           1       0.77      0.78      0.78      1071\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "best_lr = gs.best_estimator_\n",
    "\n",
    "y_test_pred = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
